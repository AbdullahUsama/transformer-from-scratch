# ðŸ§  Transformer From Scratch (PyTorch)

This repository contains my **from-scratch implementation of the Transformer architecture**, as described in the paper:

> [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€“ Vaswani et al. (2017)


I built this project to deeply understand how Transformers work under the hood, following both the original paper and selected online tutorials (listed below).

---

## ðŸ“Œ Features

- âœ… Multi-Head Self-Attention
- âœ… Positional Encoding
- âœ… Layer Normalization
- âœ… Encoder & Decoder blocks
- âœ… Scaled Dot-Product Attention
- âœ… Masking (padding & look-ahead)
- âœ… Feed-Forward Networks

---

## ðŸ“˜ Learning Resources I Used

To understand the architecture and its components, I referenced:

- The original paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- Online tutorials and blog posts such as:
  - `The Annotated Transformer` by Harvard NLP
  - Blog posts/videos by `Jay Alammar`, `Sebastian Raschka`, and others
  - PyTorch source code and documentation

---

![image](https://github.com/user-attachments/assets/55299af5-4131-4f51-a049-4c300c6adc42)

