#  Transformer From Scratch (PyTorch)

This repository contains my **from-scratch implementation of the Transformer architecture**, as described in the paper:

> [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€“ Vaswani et al. (2017)


I built this project to deeply understand how Transformers work under the hood, following both the original paper and selected online tutorials (listed below).

---

##  Features

-  Multi-Head Self-Attention
-  Positional Encoding
-  Layer Normalization
-  Encoder & Decoder blocks
-  Scaled Dot-Product Attention
-  Masking (padding & look-ahead)
-  Feed-Forward Networks

---

## ðŸ“˜ Learning Resources I Used

To understand the architecture and its components, I referenced:

- The original paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- Online tutorials and blog posts such as:
  - `The Annotated Transformer` by Harvard NLP
  - Blog posts/videos by `Jay Alammar`, `Sebastian Raschka`, and others
  - PyTorch source code and documentation

---

![image](https://github.com/user-attachments/assets/55299af5-4131-4f51-a049-4c300c6adc42)

